# Knowledge Distillation Method

- (KD) `kd` - Distilling the Knowledge in a Neural Network  
- (FitNet) `hint` - Fitnets: hints for thin deep nets  
- (AT) `attention` - Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer  
- (SP) `similarity` - Similarity-Preserving Knowledge Distillation  
(CC) `correlation` - Correlation Congruence for Knowledge Distillation  
(VID) `vid` - Variational Information Distillation for Knowledge Transfer  
(RKD) `rkd` - Relational Knowledge Distillation  
(PKT) `pkt` - Probabilistic Knowledge Transfer for deep representation learning  
(AB) `abound` - Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons  
(FT) `factor` - Paraphrasing Complex Network: Network Compression via Factor Transfer  
(FSP) `fsp` - A Gift from Knowledge Distillation:Fast Optimization, Network Minimization and Transfer Learning  
(NST) `nst` - Like what you like: knowledge distill via neuron selectivity transfer   
(CRD) `crd` - Contrastive Representation Distillation
